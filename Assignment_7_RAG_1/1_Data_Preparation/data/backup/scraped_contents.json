[
  {
    "link": "https://kubernetes.io/docs/concepts/overview/",
    "content": "Overview | Kubernetes Overview | Kubernetes Kubernetes Documentation Kubernetes Blog Training Careers Partners Community Versions Release Information v1.33 v1.32 v1.31 v1.30 v1.29 English \u09ac\u09be\u0982\u09b2\u09be (Bengali) \u4e2d\u6587 (Chinese) Fran\u00e7ais (French) Deutsch (German) \u0939\u093f\u0928\u094d\u0926\u0940 (Hindi) Bahasa Indonesia (Indonesian) Italiano (Italian) \u65e5\u672c\u8a9e (Japanese) \ud55c\uad6d\uc5b4 (Korean) Polski (Polish) Portugu\u00eas (Portuguese) \u0420\u0443\u0441\u0441\u043a\u0438\u0439 (Russian) Espa\u00f1ol (Spanish) \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 (Ukrainian) Ti\u1ebfng Vi\u1ec7t (Vietnamese) Documentation Available Documentation Versions Getting started Learning environment Production environment Container Runtimes Installing Kubernetes with deployment tools Bootstrapping clusters with kubeadm Installing kubeadm Troubleshooting kubeadm Creating a cluster with kubeadm Customizing components with the kubeadm API Options for Highly Available Topology Creating Highly Available Clusters with kubeadm Set up a High Availability etcd Cluster with kubeadm Configuring each kubelet in your cluster using kubeadm Dual-stack support with kubeadm Turnkey Cloud Solutions Best practices Considerations for large clusters Running in multiple zones Validate node setup Enforcing Pod Security Standards PKI certificates and requirements Concepts Overview Kubernetes Components Objects In Kubernetes Kubernetes Object Management Object Names and IDs Labels and Selectors Namespaces Annotations Field Selectors Finalizers Owners and Dependents Recommended Labels The Kubernetes API Cluster Architecture Nodes Communication between Nodes and the Control Plane Controllers Leases Cloud Controller Manager About cgroup v2 Kubernetes Self-Healing Container Runtime Interface (CRI) Garbage Collection Mixed Version Proxy Containers Images Container Environment Runtime Class Container Lifecycle Hooks Workloads Pods Pod Lifecycle Init Containers Sidecar Containers Ephemeral Containers Disruptions Pod Quality of Service Classes User Namespaces Downward API Workload Management Deployments ReplicaSet StatefulSets DaemonSet Jobs Automatic Cleanup for Finished Jobs CronJob ReplicationController Autoscaling Workloads Managing Workloads Services, Load Balancing, and Networking Service Ingress Ingress Controllers Gateway API EndpointSlices Network Policies DNS for Services and Pods IPv4/IPv6 dual-stack Topology Aware Routing Networking on Windows Service ClusterIP allocation Service Internal Traffic Policy Storage Volumes Persistent Volumes Projected Volumes Ephemeral Volumes Storage Classes Volume Attributes Classes Dynamic Volume Provisioning Volume Snapshots Volume Snapshot Classes CSI Volume Cloning Storage Capacity Node-specific Volume Limits Volume Health Monitoring Windows Storage Configuration Configuration Best Practices ConfigMaps Secrets Liveness, Readiness, and Startup Probes Resource Management for Pods and Containers Organizing Cluster Access Using kubeconfig Files Resource Management for Windows nodes Security Cloud Native Security Pod Security Standards Pod Security Admission Service Accounts Pod Security Policies Security For Windows Nodes Controlling Access to the Kubernetes API Role Based Access Control Good Practices Good practices for Kubernetes Secrets Multi-tenancy Hardening Guide - Authentication Mechanisms Hardening Guide - Scheduler Configuration Kubernetes API Server Bypass Risks Linux kernel security constraints for Pods and containers Security Checklist Application Security Checklist Policies Limit Ranges Resource Quotas Process ID Limits And Reservations Node Resource Managers Scheduling, Preemption and Eviction Kubernetes Scheduler Assigning Pods to Nodes Pod Overhead Pod Scheduling Readiness Pod Topology Spread Constraints Taints and Tolerations Scheduling Framework Dynamic Resource Allocation Scheduler Performance Tuning Resource Bin Packing Pod Priority and Preemption Node-pressure Eviction API-initiated Eviction Cluster Administration Node Shutdowns Node Autoscaling Certificates Cluster Networking Admission Webhook Good Practices Logging Architecture Compatibility Version For Kubernetes Control Plane Components Metrics For Kubernetes System Components Metrics for Kubernetes Object States System Logs Traces For Kubernetes System Components Proxies in Kubernetes API Priority and Fairness Installing Addons Coordinated Leader Election Windows in Kubernetes Windows containers in Kubernetes Guide for Running Windows Containers in Kubernetes Extending Kubernetes Compute, Storage, and Networking Extensions Network Plugins Device Plugins Extending the Kubernetes API Custom Resources Kubernetes API Aggregation Layer Operator pattern Tasks Install Tools Install and Set Up kubectl on Linux Install and Set Up kubectl on macOS Install and Set Up kubectl on Windows Administer a Cluster Administration with kubeadm Adding Linux worker nodes Adding Windows worker nodes Upgrading kubeadm clusters Upgrading Linux nodes Upgrading Windows nodes Configuring a cgroup driver Certificate Management with kubeadm Reconfiguring a kubeadm cluster Changing The Kubernetes Package Repository Overprovision Node Capacity For A Cluster Migrating from dockershim Changing the Container Runtime on a Node from Docker Engine to containerd Find Out What Container Runtime is Used on a Node Troubleshooting CNI plugin-related errors Check whether dockershim removal affects you Migrating telemetry and security agents from dockershim Generate Certificates Manually Manage Memory, CPU, and API Resources Configure Default Memory Requests and Limits for a Namespace Configure Default CPU Requests and Limits for a Namespace Configure Minimum and Maximum Memory Constraints for a Namespace Configure Minimum and Maximum CPU Constraints for a Namespace Configure Memory and CPU Quotas for a Namespace Configure a Pod Quota for a Namespace Install a Network Policy Provider Use Antrea for NetworkPolicy Use Calico for NetworkPolicy Use Cilium for NetworkPolicy Use Kube-router for NetworkPolicy Romana for NetworkPolicy Weave Net for NetworkPolicy Access Clusters Using the Kubernetes API Advertise Extended Resources for a Node Autoscale the DNS Service in a Cluster Change the Access Mode of a PersistentVolume to ReadWriteOncePod Change the default StorageClass Switching from Polling to CRI Event-based Updates to Container Status Change the Reclaim Policy of a PersistentVolume Cloud Controller Manager Administration Configure a kubelet image credential provider Configure Quotas for API Objects Control CPU Management Policies on the Node Control Topology Management Policies on a node Customizing DNS Service Debugging DNS Resolution Declare Network Policy Developing Cloud Controller Manager Enable Or Disable A Kubernetes API Encrypting Confidential Data at Rest Decrypt Confidential Data that is Already Encrypted at Rest Guaranteed Scheduling For Critical Add-On Pods IP Masquerade Agent User Guide Limit Storage Consumption Migrate Replicated Control Plane To Use Cloud Controller Manager Operating etcd clusters for Kubernetes Reserve Compute Resources for System Daemons Running Kubernetes Node Components as a Non-root User Safely Drain a Node Securing a Cluster Set Kubelet Parameters Via A Configuration File Share a Cluster with Namespaces Upgrade A Cluster Use Cascading Deletion in a Cluster Using a KMS provider for data encryption Using CoreDNS for Service Discovery Using NodeLocal DNSCache in Kubernetes Clusters Using sysctls in a Kubernetes Cluster Utilizing the NUMA-aware Memory Manager Verify Signed Kubernetes Artifacts Configure Pods and Containers Assign Memory Resources to Containers and Pods Assign CPU Resources to Containers and Pods Assign Pod-level CPU and memory resources Configure GMSA for Windows Pods and containers Resize CPU and Memory Resources assigned to Containers Configure RunAsUserName for Windows pods and containers Create a Windows HostProcess Pod Configure Quality of Service for Pods Assign Extended Resources to a Container Configure a Pod to Use a Volume for Storage Configure a Pod to Use a PersistentVolume for Storage Configure a Pod to Use a Projected Volume for Storage Configure a Security Context for a Pod or Container Configure Service Accounts for Pods Pull an Image from a Private Registry Configure Liveness, Readiness and Startup Probes Assign Pods to Nodes Assign Pods to Nodes using Node Affinity Configure Pod Initialization Attach Handlers to Container Lifecycle Events Configure a Pod to Use a ConfigMap Share Process Namespace between Containers in a Pod Use a User Namespace With a Pod Use an Image Volume With a Pod Create static Pods Translate a Docker Compose File to Kubernetes Resources Enforce Pod Security Standards by Configuring the Built-in Admission Controller Enforce Pod Security Standards with Namespace Labels Migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller Monitoring, Logging, and Debugging Troubleshooting Applications Debug Pods Debug Services Debug a StatefulSet Determine the Reason for Pod Failure Debug Init Containers Debug Running Pods Get a Shell to a Running Container Troubleshooting Clusters Troubleshooting kubectl Resource metrics pipeline Tools for Monitoring Resources Monitor Node Health Debugging Kubernetes nodes with crictl Auditing Debugging Kubernetes Nodes With Kubectl Developing and debugging services locally using telepresence Windows debugging tips Manage Kubernetes Objects Declarative Management of Kubernetes Objects Using Configuration Files Declarative Management of Kubernetes Objects Using Kustomize Managing Kubernetes Objects Using Imperative Commands Imperative Management of Kubernetes Objects Using Configuration Files Update API Objects in Place Using kubectl patch Migrate Kubernetes Objects Using Storage Version Migration Managing Secrets Managing Secrets using kubectl Managing Secrets using Configuration File Managing Secrets using Kustomize Inject Data Into Applications Define a Command and Arguments for a Container Define Dependent Environment Variables Define Environment Variables for a Container Expose Pod Infor"
  },
  {
    "link": "https://www.docker.com/blog/docker-for-devops/",
    "content": "Exploring Docker for DevOps: What It Is and How It Works | Docker Docs Get Support Contact Sales Products Products Docker Desktop Containerize your applications Docker Hub Discover and share container images Docker Scout Simplify the software supply chain Docker Build Cloud Speed up your image builds Testcontainers Desktop Local testing with real dependencies Testcontainers Cloud Test without limits in the cloud Docker MCP Catalog and Toolkit Connect and manage MCP tools Docker Hardened Images Ship with secure, enterprise-ready images Docker Desktop v4.43 Find out what\u2019s new to Docker Desktop in the latest release Read more Developers Developers Documentation Find guides for Docker products Getting Started Learn the Docker basics Resources Search a library of helpful materials Training Skill up your Docker knowledge Extensions SDK Create and share your own extensions Community Connect with other Docker developers Open Source Explore open source projects Preview Program Help shape the future of Docker Customer Stories Get inspired with customer stories Get the latest Docker news More resources for developers Introducing Docker Model Runner A faster, simpler way to run and test AI models locally Read more Deliver Quickly. Build Securely. Stay Competitive. Meet growing demands for speed and security with integrated, efficient solutions Read more Pricing Support Blog Company Company About Us Let us introduce ourselves What is a Container? Learn about containerization Why Docker Discover what makes us different Trust Find our customer trust resources Partners Become a Docker partner Customer Success Learn how you can succeed with Docker Events Attend live and virtual meet ups Docker Store Gear up with exclusive SWAG Careers Apply to join our team Contact Us We\u2019d love to hear from you Docker Announces SOC 2 Type 2 Attestation & ISO 27001 Certification Learn what this means for Docker security and compliance Read more Search Sign In Get Started Toggle menu Exploring Docker for DevOps: What It Is and How It Works Todd R. Weiss DevOps aims to dramatically improve the software development lifecycle by bringing together the formerly separated worlds of development and operations using principles that strive to make software creation more efficient. DevOps practices form a useful roadmap to help developers in every phase of the development lifecycle, from code planning to building, task automation, testing, monitoring, releasing, and deploying applications. As DevOps use continues to expand, many developers and organizations find that the Docker containerization platform integrates well as a crucial component of DevOps practices. Using Docker, developers have the advantage of being able to collaborate in standardized environments using local containers and remote container tools where they can write their code, share their work, and collaborate. In this blog post, we will explore the use of Docker within DevOps practices and explain how the combination can help developers create more efficient and powerful workflows. What is DevOps? DevOps practices are beneficial in the world of developers and code creation because they encourage smart planning, collaboration, and orderly processes and management throughout the software development pipeline. Without unified DevOps principles, code is typically created in individual silos that can hamper creativity, efficient management, speed, and quality. Bringing software developers, operations teams, and processes together under DevOps principles, can improve both developer and organizational efficiency through increased collaboration, agility, and innovation. DevOps brings these positive changes to organizations by constantly integrating user feedback regarding application features, shortcomings, and code glitches and \u2014 by making changes as needed on the fly \u2014 reducing operational and security risks in production code. CI/CD In addition to collaboration, DevOps principles are built around procedures for continuous integration/improvement (CI) and continuous deployment/delivery (CD) of code, shortening the cycle between development and production. This CI/CD approach lets teams more quickly adapt to feedback and thus build better applications from code conception all the way through to end-user experiences. Using CI, developers can frequently and automatically integrate their changes into the source code as they create new code, while the CD side tests and delivers those vetted changes to the production environment. By integrating CI/CD practices, developers can create cleaner and safer code and resolve bugs ahead of production through automation, collaboration, and strong QA pipelines. What is Docker? The Docker containerization platform is a suite of tools, standards, and services that enable DevOps practices for application developers. Docker is used to develop, ship, and run applications within lightweight containers. This approach allows developers to separate their applications from their business infrastructure, giving them the power to deliver better code more quickly. The Docker platform enables developers to package and run their application code in lightweight, local, standardized containers, which provide a loosely isolated environment that contains everything needed to run the application \u2014 including tools, packages, and libraries. By using Docker containers on a Docker client, developers can run an application without worrying about what is installed on the host, giving them huge flexibility, security, and collaborative advantages over virtual machines. In this controlled environment, developers can use Docker to create, monitor, and push their applications into a test environment, run automated and manual tests as needed, correct bugs, and then validate the code before deploying it for use in production. Docker also allows developers to run many containers simultaneously on a host, while allowing those same containers to be shared with others. Such a collaborative workspace can foster healthy and direct communications between developers, allowing development processes to become easier, more accurate, and more secure. Containers vs. virtualization Containers are an abstraction that packages application code and dependencies together. Instances of the container can then be created, started, stopped, moved, or deleted using the Docker API or command-line interface (CLI). Containers can be connected to one or more networks, be attached to storage, or create new images based on their current states. Containers differ from virtual machines, which use a software abstraction layer on top of computer hardware, allowing the hardware to be shared more efficiently in multiple instances that will run individual applications. Docker containers require fewer physical hardware resources than virtual machines, and they also offer faster startup times and lower overhead. This makes Docker ideal for high-velocity environments, where rapid software development cycles and scalability are crucial. Basic components of Docker The basic components of Docker include: Docker images: Docker images are the blueprints for your containers. They are read-only templates that contain the instructions for creating a Docker container. You can think of a container image as a snapshot of a specific state of your application. Containers: Containers are the instances of Docker images. They are lightweight and portable, encapsulating your application along with its dependencies. Containers can be created, started, stopped, moved, and deleted using simple Docker commands. Dockerfiles: A Dockerfile is a text document containing a series of instructions on how to build a Docker image. It includes commands for specifying the base image, copying files, installing dependencies, and setting up the environment. Docker Engine : Docker Engine is the core component of Docker. It\u2019s a client-server application that includes a server with a long-running daemon process, APIs for interacting with the daemon, and a CLI client. Docker Desktop : Docker Desktop is a commercial product sold and supported by Docker, Inc. It includes the Docker Engine and other open source components, proprietary components, and features like an intuitive GUI, synchronized file shares, access to cloud resources, debugging features, native host integration, governance, security features, and administrative settings management. Docker Hub: Docker Hub is a public registry where you can store and share Docker images. It serves as a central place to find official Docker images and user-contributed images. You can also use Docker Hub to automate your workflows by connecting it to your CI/CD pipelines. Basic Docker commands Docker commands are simple and intuitive. For example: docker run : Runs a Docker container from a specified image. For example, docker run hello-world will run a container from the \u201chello-world\u201d image. docker build : Builds an image from a Dockerfile. For example, docker build -t my-app . will build an image named \u201cmy-app\u201d from the Dockerfile in the current directory. docker pull : Pulls an image from Docker Hub. For example, docker pull nginx will download the latest NGINX image from Docker Hub. docker ps : Lists all running containers. For example, docker ps -a will list all containers, including stopped ones. docker stop : Stops a running Docker container. For example, docker stop <container_id> will stop the container with the specified ID. docker rm : Removes a stopped container. For example, docker rm <container_id> will remove the container with the specified ID. How Docker is used in DevOps One of Docker\u2019s most important benefits for developers is its critical role in facilitating CI/CD in the application development process. This makes it easier and more seamless for developers to work together to create better code. Docker is a build environment where developers can get predictable results building and testing their applicat"
  },
  {
    "link": "https://mlflow.org/docs/latest/ml/",
    "content": "MLflow: A Tool for Managing the Machine Learning Lifecycle | MLflow Skip to main content ML Docs ML Docs GenAI Docs API Reference GitHub Search MLflow MLflow 3.0 Getting Started \u00f0\u009f\u009a\u0080 Machine Learning \u00f0\u009f\u00a4\u0096 Traditional ML Deep Learning \u00f0\u009f\u0095\u00b8\u00ef\u00b8\u008f Build \u00f0\u009f\u0094\u00a8 MLflow Tracking \u00f0\u009f\u0093\u0088 MLflow Model \u00f0\u009f\u00a7 MLflow Datasets \u00f0\u009f\u0097\u0083\u00ef\u00b8\u008f Evaluate \u00f0\u009f\u008e\u00af Deploy \u00f0\u009f\u009a\u00a2 Team Collaboration \u00f0\u009f\u0091\u00a5 API References More MLflow On this page MLflow: A Tool for Managing the Machine Learning Lifecycle MLflow is an open-source platform, purpose-built to assist machine learning practitioners and teams in\nhandling the complexities of the machine learning process. MLflow focuses on the full lifecycle for\nmachine learning projects, ensuring that each phase is manageable, traceable, and reproducible. MLflow Getting Started Resources \u00e2\u0080\u008b If this is your first time exploring MLflow, the tutorials and guides here are a great place to start. The emphasis in each of these is\ngetting you up to speed as quickly as possible with the basic functionality, terms, APIs, and general best practices of using MLflow in order to\nenhance your learning in area-specific guides and tutorials. Learn about MLflow MLflow Basics MLflow Models Introduction Traditional ML Deep Learning Learn about the core components of MLflow \u00e2\u0080\u008b Quickstarts \u00e2\u0080\u008b Get Started with MLflow in our 5-minute tutorial Guides \u00e2\u0080\u008b Learn the core components of MLflow with this in-depth guide to Tracking Learn how to perform common tasks in MLflow \u00e2\u0080\u008b Guides \u00e2\u0080\u008b Autologging guide for effortless model tracking Model Signatures and type validation in MLflow Model Deployment Quickstart Hyperparameter tuning with MLflow Learn about MLflow Model-related topics \u00e2\u0080\u008b Guides \u00e2\u0080\u008b Introduction to Custom Python Models Model dependency management in MLflow Model Signatures and type validation Get started with MLflow's Traditional ML integrations \u00e2\u0080\u008b Quickstarts and Guides \u00e2\u0080\u008b Scikit-learn introduction with MLflow XGBoost usage with MLflow Spark ML guide Prophet Time Series Forecasting with MLflow Get started with MLflow's Deep Learning Library integrations \u00e2\u0080\u008b Guides and Tutorials \u00e2\u0080\u008b TensorFlow PyTorch Keras spaCy for NLP workflows Traditional ML and Deep Learning with MLflow \u00e2\u0080\u008b MLflow provides comprehensive support for traditional machine learning and deep learning workflows. From experiment tracking and model versioning to deployment and monitoring, MLflow streamlines every aspect of the ML lifecycle. Whether you're working with scikit-learn models, training deep neural networks, or managing complex ML pipelines, MLflow provides the tools you need to build reliable, scalable machine learning systems. Explore the core MLflow capabilities and integrations below to enhance your ML development workflow! Tracking & Experiments Model Registry Model Deployment ML Library Integrations Model Evaluation Track experiments and manage your ML development \u00e2\u0080\u008b Core Features \u00e2\u0080\u008b MLflow Tracking provides comprehensive experiment logging, parameter tracking, metrics visualization, and artifact management. Key Benefits: \u00e2\u0080\u00a2 Experiment Organization : Track and compare multiple model experiments \u00e2\u0080\u00a2 Metric Visualization : Built-in plots and charts for model performance \u00e2\u0080\u00a2 Artifact Storage : Store models, plots, and other files with each run \u00e2\u0080\u00a2 Collaboration : Share experiments and results across teams Guides \u00e2\u0080\u008b Getting Started with Tracking Advanced Tracking Features Autologging for Popular Libraries Manage model versions and lifecycle \u00e2\u0080\u008b Core Features \u00e2\u0080\u008b MLflow Model Registry provides centralized model versioning, stage management, and model lineage tracking. Key Benefits: \u00e2\u0080\u00a2 Version Control : Track model versions with automatic lineage \u00e2\u0080\u00a2 Stage Management : Promote models through staging, production, and archived stages \u00e2\u0080\u00a2 Collaboration : Team-based model review and approval workflows \u00e2\u0080\u00a2 Model Discovery : Search and discover models across your organization Guides \u00e2\u0080\u008b Model Registry Introduction Registering Your First Model Deploy models to production environments \u00e2\u0080\u008b Core Features \u00e2\u0080\u008b MLflow Deployment supports multiple deployment targets including REST APIs, cloud platforms, and edge devices. Key Benefits: \u00e2\u0080\u00a2 Multiple Targets : Deploy to local servers, cloud platforms, or containerized environments \u00e2\u0080\u00a2 Model Serving : Built-in REST API serving with automatic input validation \u00e2\u0080\u00a2 Batch Inference : Support for batch scoring and offline predictions \u00e2\u0080\u00a2 Production Ready : Scalable deployment options for enterprise use Guides \u00e2\u0080\u008b Model Deployment Overview Local Model Serving Cloud Deployment Options Kubernetes Deployment Explore Native MLflow ML Library Integrations \u00e2\u0080\u008b Scikit-learn XGBoost TensorFlow PyTorch Keras Spark MLlib Evaluate and validate your ML models \u00e2\u0080\u008b Core Features \u00e2\u0080\u008b MLflow Evaluation provides comprehensive model validation tools, automated metrics calculation, and model comparison capabilities. Key Benefits: \u00e2\u0080\u00a2 Automated Metrics : Built-in evaluation metrics for classification, regression, and more \u00e2\u0080\u00a2 Custom Evaluators : Create custom evaluation functions for domain-specific metrics \u00e2\u0080\u00a2 Model Comparison : Compare multiple models and versions side-by-side \u00e2\u0080\u00a2 Validation Datasets : Track evaluation datasets and ensure reproducible results Guides \u00e2\u0080\u008b Learn how to evaluate your ML models with MLflow Discover custom evaluation metrics and functions Compare models with MLflow Model Comparison Running MLflow Anywhere \u00e2\u0080\u008b MLflow can be used in a variety of environments, including your local environment, on-premises clusters, cloud platforms, and managed services. Being an open-source platform, MLflow is vendor-neutral ; no matter where you are doing machine learning, you have access to the MLflow's core capabilities sets such as tracking, evaluation, observability, and more. Hosting MLflow Locally Run MLflow server locally or use direct access mode (no server required) to run MLflow in your local environment. Click the card to learn more. Databricks Managed MLflow is a FREE, fully managed solution, seamlessly integrated with Databricks ML/AI ecosystem, such as Unity Catalog, Model Serving, and more. MLflow on Amazon SageMaker is a fully managed service for MLflow on AWS infrastructure,integrated with SageMaker's core capabilities such as Studio, Model Registry, and Inference. Azure Machine Learning workspaces are MLflow-compatible, allows you to use an Azure Machine Learning workspace the same way you use an MLflow server. Nebius, a cutting-edge cloud platform for GenAI explorers, offers a fully managed service for MLflow, streamlining LLM fine-tuning with MLflow's robust experiment tracking capabilities. You can use MLflow on your on-premise or cloud-managed Kubernetes cluster. Click this card to learn how to host MLflow on your own infrastructure. Next Getting Started with MLflow MLflow Getting Started Resources Learn about the core components of MLflow Learn how to perform common tasks in MLflow Learn about MLflow Model-related topics Get started with MLflow's Traditional ML integrations Get started with MLflow's Deep Learning Library integrations Traditional ML and Deep Learning with MLflow Track experiments and manage your ML development Manage model versions and lifecycle Deploy models to production environments Explore Native MLflow ML Library Integrations Evaluate and validate your ML models Running MLflow Anywhere \u00c2\u00a9 2025 MLflow Project, a Series of LF Projects, LLC. Components Releases Blog Docs Ambassador Program"
  },
  {
    "link": "https://pytorch.org/",
    "content": "PyTorch Skip to main content github Join us at PyTorch Conference in San Francisco, October 22-23. Register now! Hit enter to search or ESC to close Close Search search Menu Learn Get Started Tutorials Learn the Basics PyTorch Recipes Intro to PyTorch \u2013 YouTube Series Webinars Community Landscape Join the Ecosystem Community Hub Forums Developer Resources Contributor Awards Community Events PyTorch Ambassadors Projects PyTorch vLLM DeepSpeed Host Your Project Docs PyTorch Domains Blog & News Blog Announcements Case Studies Events Newsletter About PyTorch Foundation Members Governing Board Technical Advisory Council Cloud Credit Program Staff Contact JOIN search Get Started Choose Your Path: Install PyTorch Locally or Launch Instantly on Supported Cloud Platforms Get started July 2, 2025 in Blog Reducing Storage Footprint and Bandwidth Usage for Distributed Checkpoints with PyTorch DCP Summary PyTorch Distributed Checkpointing (DCP) is a versatile and powerful tool for managing model checkpoints in distributed training environments. Its modular design empowers developers to tailor its components to their\u2026 Read More June 25, 2025 in Blog PyTorch + vLLM = \u2665\ufe0f Key takeaways: PyTorch and vLLM are both critical to the AI ecosystem and are increasingly being used together for cutting-edge generative AI applications, including inference, post-training, and agentic systems at\u2026 Read More June 25, 2025 in Blog , Ecosystem FlagGems Joins the PyTorch Ecosystem: Triton-Powered Operator Library for Universal AI Acceleration In the race to accelerate large language models across diverse AI hardware, FlagGems delivers a high-performance, flexible, and scalable solution. Built on Triton language, FlagGems is a plugin-based PyTorch operator\u2026 Read More Join PyTorch Foundation As a member of the PyTorch Foundation, you\u2019ll have access to resources that allow you to be stewards of stable, secure, and long-lasting codebases. You can collaborate on training, local and regional events, open-source developer tooling, academic research, and guides to help new users and contributors have a productive experience. EXPLORE BENEFITS Key Features & Capabilities Production Ready Transition seamlessly between eager and graph modes with TorchScript, and accelerate the path to production with TorchServe. Distributed Training Scalable distributed training and performance optimization in research and production is enabled by the torch.distributed backend. Robust Ecosystem A rich ecosystem of tools and libraries extends PyTorch and supports development in computer vision, NLP and more. Cloud Support PyTorch is well supported on major cloud platforms, providing frictionless development and easy scaling. Install PyTorch Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. This should be suitable for many users. Preview is available if you want the latest, not fully tested and supported, builds that are generated nightly. Please ensure that you have met the prerequisites below (e.g., numpy) , depending on your package manager. Anaconda is our recommended package manager since it installs all dependencies. You can also install previous versions of PyTorch . Note that LibTorch is only available for C++. NOTE: Latest PyTorch requires Python 3.9 or later. PyTorch Build Your OS Package Language Compute Platform Run this Command: PyTorch Build Stable (2.7.0) Preview (Nightly) Your OS Linux Mac Windows Package Pip LibTorch Source Language Python C++ / Java Compute Platform CUDA 11.8 CUDA 12.6 CUDA 12.8 ROCm 6.3 CPU Run this Command: pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 Previous versions of PyTorch Quick Start With Cloud Partners Get up and running with PyTorch quickly through popular cloud platforms and machine learning services. Amazon Web Services PyTorch on AWS Amazon SageMaker AWS Deep Learning Containers AWS Deep Learning AMIs Google Cloud Platform Cloud Deep Learning VM Image Deep Learning Containers Microsoft Azure PyTorch on Azure Azure Machine Learning Azure Functions Lightning Studios lightning.ai Ecosystem BROWSE PROJECTS Featured Projects Explore a rich ecosystem of libraries, tools, and more to support development. Captum Captum (\u201ccomprehension\u201d in Latin) is an open source, extensible library for model interpretability built on PyTorch. PyTorch Geometric PyTorch Geometric is a library for deep learning on irregular input data such as graphs, point clouds, and manifolds. skorch skorch is a high-level library for PyTorch that provides full scikit-learn compatibility. Companies & Universities Using PyTorch Amazon Advertising Reduce inference costs by 71% and scale out using PyTorch, TorchServe, and AWS Inferentia. READ CASE STUDIES Salesforce Pushing the state of the art in NLP and Multi-task learning. Stanford University Using PyTorch\u2019s flexibility to efficiently research new algorithmic approaches. Docs Access comprehensive developer documentation for PyTorch View Docs \u203a Tutorials Get in-depth tutorials for beginners and advanced developers View Tutorials \u203a Resources Find development resources and get your questions answered View Resources \u203a Stay in touch for updates, event info, and the latest news By submitting this form, I consent to receive marketing emails from the LF and its projects regarding their events, training, research, developments, and related announcements. I understand that I can unsubscribe at any time using the links in the footers of the emails I receive. Privacy Policy . x-twitter facebook linkedin youtube github slack wechat \u00a9 2025 PyTorch. Copyright \u00a9 The Linux Foundation\u00ae. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark usage, please see our Policies page. Trademark Usage . Privacy Policy . Close Menu Join us at PyTorch Conference in San Francisco, October 22-23. Register now! Learn Get Started Tutorials Learn the Basics PyTorch Recipes Intro to PyTorch \u2013 YouTube Series Webinars Community Landscape Join the Ecosystem Community Hub Forums Developer Resources Contributor Awards Community Events PyTorch Ambassadors Projects PyTorch vLLM DeepSpeed Host Your Project Docs PyTorch Domains Blog & News Blog Announcements Case Studies Events Newsletter About PyTorch Foundation Members Governing Board Technical Advisory Council Cloud Credit Program Staff Contact JOIN github"
  },
  {
    "link": "https://openai.com/index/why-our-structure-must-evolve-to-advance-our-mission/",
    "content": "Enable JavaScript and cookies to continue"
  }
]